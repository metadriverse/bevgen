<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title> BEVGen: Learning to Generate Diverse and Realistic Traffic Scenarios</title>
    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>
<!-- === Header Ends === -->

<script>
    var lang_flag = 1;
</script>

<body>

<!-- === Home Section Starts === -->
<div class="section" style="margin-top: 15pt">
    <!-- === Title Starts === -->
    <div class="header">
        <table>
            <tr>
                <td>
                    <div style="padding-top: 0pt;padding-left: 140pt;padding-bottom: 12pt" class="title" id="lang">
                        <b> BEVGen: Learning to Generate Diverse<br>
                            and Realistic Traffic Scenarios </b>
                    </div>

                </td>
            </tr>
        </table>


    </div>
    <!-- === Title Ends === -->
    <div class="author">
        <!-- <p style="text-align:center">IEEE International Conference on Robotics and Automation (ICRA) 2023</p> -->
        <a href="https://aswerdlow.com" target="_blank">Alexander Swerdlow</a>,
        <a href="https://Quanyili.github.io">Runsheng Xu</a>,&nbsp;
        <a href="https://pengzhenghao.github.io" target="_blank">Bolei Zhou</a>&nbsp;&nbsp;
    </div>

    <div class="institution" style="font-size: 11pt;">
        <div>
         University of California, Los Angeles
        </div>
    </div>
    <table border="0" align="center">
        <tr>
            <td align="center" style="padding: 0pt 0 15pt 0">
                <a class="bar" href="https://metadriverse.github.io/trafficgen/"><b>Webpage</b></a> |
                <!-- <a class="bar" href="https://youtu.be/jPS93-d6msM"><b>Video</b></a> | -->
                <a class="bar" href="https://github.com/alexanderswerdlow/BEVGen"><b>Code</b></a> |
                <a class="bar" href="https://arxiv.org/abs/2301.04634"><b>Paper</b></a>
            </td>
        </tr>
    </table>
    <center>
            <img src="assets/argoverse_intro.png" alt="Image 1" width="100%" style="margin-bottom: 10px;">
            <img src="assets/nuscenes_intro.png" alt="Image 2" width="100%%">
    </center>
</div>


<!-- === Overview Section Starts === -->
<div class="section">
    <div class="title" id="method">Overview</div>
    <div class="body">
        <div class="teaser">
            <img src="assets/figure_1.png">
            <div class="text">
                <br>
                Fig. 1 Overview of the proposed method
            </div>
        </div>
        <div class="text">
            <p>
                In this work, we tackle the new task of generating street-view images from a BEV layout and propose a generative model called BEVGen to address the underlying challenges. We develop an autoregressive neural model called BEVGen that generates a set of realistic and spatially consistent images. BEVGen has two technical novelties: (i) it incorporates spatial embeddings using camera instrinsics and extrinsics to allow the model to attend to relevant portions of the images and HD map, and (ii) it contains a novel attention bias and decoding scheme that maintains both image consistency and correspondence.
            </p>

            <div class="teaser" style="width: 105%; margin-left: -25pt">
                <img src="assets/comp_28_ring_front_center_ring_front_right__0.png">
                <img src="assets/comp_28_ring_front_center_ring_front_right__1.png">
                <div class="text">
                    <br>
                    Fig. 2: We display correspondences across views. Green lines indicate inliers obtained using RANSAC for filtering. Top: Real images, Bottom: Synthesized images.
                </div>
            </div>

            <!-- <p>
                TrafficGen synthesizes a complete traffic scenario in an autoregressive way.
                The first step is to place vehicles on the given map.
                At each iteration, a vehicle is placed by sampling the spatial probability distribution over all the
                regions in the scene. We visualize current spatial distribution through the iteration-varying heatmap in the above figure.
At              the end of sampling, a set of N vehicles are placed at the map and forms the traffic snapshot.
            </p>
            <p>
                We then use a motion forecasting model as the trajectory generator since it can output multi-mode diverse
                samples. However, motion forecasting models cannot be directly used for long trajectory generation,
                since they are brittle to distributional shift. We sample one possible future trajectory for each
                vehicle. To mitigate the long-term cumulative error, only the first several steps of the trajectory
                are used. Then the long trajectories of a vehicle are generated by several rollouts.
            </p> -->
        </div>
    </div>
</div>


<!-- <div class="section" style="">
    <div class="title" id="Parkour Demo">Generating diverse traffic scenarios</div>
    <p>
        With different HD maps and traffic light signals as input, TrafficGen can generate diverse traffic scenarios in the video below.
    </p>
    <center>
        <video class="video-container" width="100%" height="300" style="" autoplay
               muted loop id="parkour_video">
            <source src="assets/Motion_Primitive_Grids.mp4" type=video/mp4>
        </video>
    </center>
    <p>
        The following video shows that by sampling many times on the same map, TrafficGen can generate diverse
        traffic flows.
    </p>
    <center>
        <video class="video-container" width="100%" height="300" style="padding-right: 10pt" autoplay
               muted loop id="parkour_demo">
            <source src="assets/parkour.mp4" type=video/mp4>
        </video>
    </center>
</div> -->

<div class="section" style=" text-align: left">
    <div class="title" id="Demo Video">Demo Video</div>
    To further demonstrate the spatial disentanglement of the model, we compare the generated images to the source images over multiple frames from the same scene. We use the original BEV layouts from the validation set, and create a video by appending each generation. We observe that cars and road markings generally stay consistent between frames, with the layout visually matching the source images. Note that our model does not enforce temporal consistency, and thus it is expected that the generated frames may not produce the same vehicles and background scenery in two adiacent frames. We leave incorporating temporal consistency for future work.
    &nbsp;

    <div class="body">
        <iframe style="margin-top: 30px;" width="900" height="600" src="https://www.youtube.com/embed/8AFKpCFwwOo" title="BEVGen â€” Continuous Scene Generation" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
    </div>
</div>

<!-- === Reference Section Starts === -->
<div class="section">
    <div class="bibtex">
        <div class="text">Reference</div>
    </div>
    <pre>
@article{swerdlow2023streetview,
    title={Street-View Image Generation from a Bird's-Eye View Layout}, 
    author={Alexander Swerdlow and Runsheng Xu and Bolei Zhou},
    year={2023},
    journal={arXiv preprint arXiv:2301.04634},
}
    </pre>
    <!-- Adjust the frame size based on the demo (Every project differs). -->
</div>

</body>
</html>
